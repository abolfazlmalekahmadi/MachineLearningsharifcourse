{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abolfazl malekahmadi \n",
    "\n",
    "401205167\n",
    "\n",
    "HM1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1 Linear Regression</h3>\n",
    "\n",
    "<h4>1.1</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we are training a linear regression model with one feature $x_j$, the model can be written as:\n",
    "\n",
    "$$\\hat{y} = w_j x_j$$\n",
    "\n",
    "where $\\hat{y}$ is the predicted target value, and $w_j$ is the weight (or coefficient) assigned to the feature $x_j$.\n",
    "\n",
    "The goal of linear regression is to find the weight vector $\\mathbf{w} = [w_1, \\ldots, w_m]$ that minimizes the mean squared error between the predicted values and the actual target values. Mathematically, we can write this as:\n",
    "\n",
    "$$\\min_{\\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "or\n",
    "$$\\min_{\\mathbf{w}} \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "Substituting $\\hat{y} = w_j x_j$ and expanding the squared term, we get:\n",
    "\n",
    "$$\\min_{w_j} \\frac{1}{n} \\sum_{i=1}^n (w_j x_j^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "Taking the derivative with respect to $w_j$ and setting it to zero gives:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_j} \\frac{1}{n} \\sum_{i=1}^n (w_j x_j^{(i)} - y^{(i)})^2 = 0$$\n",
    "\n",
    "Simplifying this expression, we get:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n 2x_j^{(i)} (w_j x_j^{(i)} - y^{(i)}) = 0$$\n",
    "\n",
    "Expanding the product and rearranging, we get:\n",
    "\n",
    "$$w_j \\frac{1}{n} \\sum_{i=1}^n (x_j^{(i)})^2 - \\frac{1}{n} \\sum_{i=1}^n x_j^{(i)} y^{(i)} = 0$$\n",
    "\n",
    "Solving for $w_j$, we get:\n",
    "\n",
    "$$w_j = \\frac{\\frac{1}{n} \\sum_{i=1}^n x_j^{(i)} y^{(i)}}{\\frac{1}{n} \\sum_{i=1}^n (x_j^{(i)})^2}$$\n",
    "\n",
    "which can be simplified as:\n",
    "\n",
    "$$w_j = \\frac{x_j^T y}{x_j^T x_j}$$\n",
    "\n",
    "This is the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the columns of matrix X are orthogonal, then $X^TX$ is a diagonal matrix where the diagonal entries are the squared norms of the columns of X. That is:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\Vert x^{(1)}\\Vert^2 & 0 & \\cdots & 0 \n",
    "0 & \\Vert x^{(2)}\\Vert^2 & \\cdots & 0 \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \n",
    "0 & 0 & \\cdots & \\Vert x^{(m)}\\Vert^2\n",
    "\\end{bmatrix}$$\n",
    "where $m$ is the number of features.\n",
    "The optimal parameters for linear regression can be obtained by solving the normal equation:\n",
    "$$(X^TX)w = X^Ty$$\n",
    "Multiplying both sides by $(X^TX)^{-1}$, we get:\n",
    "$$w = (X^TX)^{-1}X^Ty$$\n",
    "Since $X^TX$ is a diagonal matrix, its inverse is also a diagonal matrix where the diagonal entries are the reciprocal of the corresponding diagonal entries in $X^TX$:\n",
    "$$(X^TX)^{-1} = \\begin{bmatrix}\n",
    "\\frac{1}{\\Vert x^{(1)}\\Vert^2} & 0 & \\cdots & 0 \\\\\n",
    "0 & \\frac{1}{\\Vert x^{(2)}\\Vert^2} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{1}{\\Vert x^{(m)}\\Vert^2}\n",
    "\\end{bmatrix}$$\n",
    "Therefore, the optimal parameters $w$ can be written as:\n",
    "$$w = \\begin{bmatrix}\n",
    "\\frac{x^{(1)T}y}{\\Vert x^{(1)}\\Vert^2} \\\\\n",
    "\\frac{x^{(2)T}y}{\\Vert x^{(2)}\\Vert^2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{x^{(m)T}y}{\\Vert x^{(m)}\\Vert^2}\n",
    "\\end{bmatrix}$$\n",
    "Notice that each entry in $w$ is the same as the weight obtained from training the regressor on each feature independently, which is:\n",
    "$$w_j = \\frac{x_j^Ty}{x_j^Tx_j}$$\n",
    "where $x_j$ is the jth column of $X$.\n",
    "Therefore, if the columns of matrix X are orthogonal, the optimal parameters obtained from training the regressor on all features are the same as the optimal parameters resulting from training on each feature independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PCA</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that $\\left|\\hat{x_i}-\\hat{x_j}\\right|=\\left|z_i-z_j\\right|$:\n",
    "\n",
    "First, we can express $\\hat{x}_i$ and $\\hat{x}_j$ in terms of $z_i$ and $z_j$ respectively as:\n",
    "$$\n",
    "\\hat{x}_i=V_{1: k} z_i \\quad \\text { and } \\quad \\hat{x}_j=V_{1: k} z_j\n",
    "$$\n",
    "Then, we can find the difference between $\\hat{x}_i$ and $\\hat{x}_j$ as:\n",
    "$$\n",
    "\\hat{x}_i-\\hat{x}_j=V_{1: k} z_i-V_{1: k} z_j=V_{1: k}\\left(z_i-z_j\\right)\n",
    "$$\n",
    "Taking the norm of both sides, we get:\n",
    "$$\n",
    "\\left\\|\\hat{x}_i-\\hat{x}_j\\right\\|=\\left\\|V_{1: k}\\left(z_i-z_j\\right)\\right\\|\n",
    "$$\n",
    "Now, using the fact that $V_{1: k}$ is an orthogonal matrix, we have:\n",
    "$$\n",
    "\\left\\|\\hat{x}_i-\\hat{x}_j\\right\\|=\\left\\|V_{1: k}\\right\\| \\cdot\\left\\|z_i-z_j\\right\\|\n",
    "$$\n",
    "Since $V_{1: k}$ is an orthonormal matrix, its norm is equal to 1, hence:\n",
    "$$\n",
    "\\left\\|\\hat{x}_i-\\hat{x}_j\\right\\|=\\left\\|z_i-z_j\\right\\|\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.2</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the derivation of the expression for the reconstruction error, we continue from where we left off:\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\left(\\sum_{i=1}^nx_i^Tv_j\\right)^2/\\left|v_j\\right|_2^2 &= \\sum{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left(\\sum_{i=1}^nx_i^Tv_j\\right)^2}{\\lambda_j} \\\n",
    "&= \\sum_{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{z_{1j}^2+\\cdots+z_{nj}^2}{\\lambda_j} \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\\n",
    "&= \\sum_{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left|Z_{1:k,j}\\right|_2^2}{\\lambda_j} \\\n",
    "&= \\sum{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left|Z_{j}\\right|_2^2}{\\lambda_j},\n",
    "\\end{align*}\n",
    "where $ZÙ€{1:k,j}$ is the $j$-th column of the matrix $Z$ and $Z_j$ is the vector obtained by taking the first $k$ entries of the $j$-th eigenvector of the covariance matrix.\n",
    "\n",
    "Now, since the eigenvalues of the covariance matrix are ordered in descending order, we can rewrite the expression for the reconstruction error as follows:\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n\\left|x_i-\\hat{x_i}\\right|_2^2 &= \\sum{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left|Z_{j}\\right|_2^2}{\\lambda_j} \\\n",
    "&= \\sum{j=1}^p\\frac{\\left|Z_{j}\\right|_2^2}{\\lambda_j} - \\sum{i=1}^nx_i^Tx_i \\\n",
    "&= \\sum_{j=1}^p\\frac{1}{\\lambda_j}\\sum_{i=1}^nx_i^Tv_jv_j^Tx_i - \\sum_{i=1}^nx_i^Tx_i \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\\n",
    "&= \\sum_{j=1}^p\\frac{1}{\\lambda_j}\\left(\\sum_{i=1}^n(v_j^Tx_i)^2\\right) - \\sum_{i=1}^nx_i^Tx_i \\\n",
    "&= \\sum_{j=k+1}^p\\frac{1}{\\lambda_j}\\left(\\sum_{i=1}^n(v_j^Tx_i)^2\\right) \\\n",
    "&= (n-1) \\Sigma_{j=k+1}^p \\lambda_j,\n",
    "\\end{align*}\n",
    "where the last step follows from the fact that $\\sum_{i=1}^n(v_j^Tx_i)^2$ is the sum of squares of the projections of the data points onto the $j$-th eigenvector, which has a chi-square distribution with $n-1$ degrees of freedom.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K-means</h3>\n",
    "\n",
    "<h4>3.1</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign each data point to the nearest cluster center (using Euclidean distance).\n",
    "\n",
    "Calculate the mean of all data points assigned to each cluster. This gives the new coordinates for the cluster centers.\n",
    "\n",
    "Repeat steps 1 and 2 until convergence (i.e. until the cluster assignments no longer change or a maximum number of iterations is reached).\n",
    "\n",
    "For the given example with 5 data points and 2 initial cluster centers, we can initialize the algorithm as follows:\n",
    "\n",
    "\n",
    "Assign the first 3 data points to cluster center 1 and the last 2 data points to cluster center 2 (this is somewhat arbitrary, but reasonable given the initial positions of the cluster centers).\n",
    "\n",
    "\n",
    "Calculate the mean of the data points in each cluster to get the new cluster centers.\n",
    "\n",
    "\n",
    "First iteration:\n",
    "\n",
    "\n",
    "\n",
    "After initialization, the first cluster center is at (4.5, 2) and the second cluster center is at (10, 4).\n",
    "Calculate the mean of each cluster to get the new cluster centers: (3.5, 10.16) and (7.75, 6.5) when show blak square .'\n",
    "Assign each data point to the nearest cluster center: [3, 10], [3.5, 9], [4, 11.5] to cluster 1, [6.5, 12], [9, 1] to cluster 2.\n",
    "\n",
    "\n",
    "Second iteration:\n",
    "\n",
    "\n",
    "\n",
    "After the first iteration, the first cluster center ha moved to  (4.25, 10.62) but the second cluster center has moved to (9, 1).\n",
    "Assign each data point to the nearest cluster center: [3, 10], [3.5, 9], [4, 11.5] and to [6.5, 12] cluster 1, [9, 1] to cluster 2.\n",
    "At this stage, the algorithm converged and ends\n",
    "\n",
    "\n",
    "<h5 style = 'color:red;'> these numbers were considered hypothetical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First iteration of K-means:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second iteration of K-means:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[3, 10], [3.5, 9], [4, 11.5], [6.5, 12], [9, 1]])\n",
    "\n",
    "centers = np.array([[4.5, 2], [10, 4]])\n",
    "\n",
    "\n",
    "print('First iteration of K-means:')\n",
    "for i in range(2):\n",
    "    distances = np.sqrt(((X - centers[:, np.newaxis])**2).sum(axis=2))\n",
    "    labels = np.argmin(distances, axis=0)\n",
    "    centers = np.array([X[labels == k].mean(axis=0) for k in range(len(centers))])\n",
    "\n",
    "    \n",
    "\n",
    "    plt.scatter(X[labels == 0, 0], X[labels == 0, 1], color='r')\n",
    "    plt.scatter(X[labels == 1, 0], X[labels == 1, 1], color='b')\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='s', color='k')\n",
    "    for j in range(i):\n",
    "        print(\"Second iteration of K-means:\")\n",
    "        x = [3, 4, 5, 6, 7,8,9,10,11,12,13,14]\n",
    "        y=[6,6,6,6,6,6,6,6,6,6,6,6]\n",
    "\n",
    "        indices = np.arange(len(x))\n",
    "        plt.plot(indices,y)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we have 2 clusters: 4 red dots are one cluster and 1 blue dot below (black square at the bottom of the page ) is another cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of hierarchical clustering over K-means is that it does not require a priori specification of the number of clusters to be formed. In hierarchical clustering, the dendrogram can be visually inspected to identify natural clusters, which may not be immediately apparent in K-means. Furthermore, hierarchical clustering allows for nested clusters, where smaller clusters are contained within larger ones, which may be more appropriate for certain datasets.  On the other hand, one advantage of K-means over hierarchical clustering is that it is computationally more efficient, particularly for large datasets. K-means is a simpler algorithm that requires fewer computations than hierarchical clustering, which can make it more feasible to use with larger datasets. Additionally, K-means can be more effective when the clusters are well-separated and of similar size and density, as it optimizes the distance between the centroids and the data points, whereas hierarchical clustering may not be able to capture such nuances in the data.  hierarchical clustering can be more flexible and suitable for datasets with complex structures and without a clear number of clusters, while K-means can be more computationally efficient and effective for datasets with well-separated, evenly sized and dense clusters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4 Gaussian Mixture Model (GMM)</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.1</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the algorithms finds the clusters well\n",
    "\n",
    "\n",
    "The difference is K-means is a hard clustering algorithm where each data point is assigned to a single cluster with a hard assignment. This means that the data point is assigned to only one cluster with a probability of 1.0, and the centroid of each cluster is calculated based on the average of all the data points assigned to that cluster.  On the other hand, GMM is a soft clustering algorithm where each data point is assigned to multiple clusters with a soft assignment. This means that each data point has a non-zero probability of belonging to each cluster, and the centroid of each cluster is calculated as the weighted average of all the data points, where the weight is the probability of the data point belonging to that cluster.  In summary, while both k-means and GMM are clustering algorithms, they differ in their approach to assigning data points to clusters and calculating the centroids of each cluster. K-means uses a hard assignment and calculates the centroid based on the average of the data points in each cluster, while GMM uses a soft assignment and calculates the centroid as a weighted average of all the data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.2</h3>\n",
    "<h4>4.2.1</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='2.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.2.2</h4>\n",
    "\n",
    "increase. Unless the EM algorithm is in local optimum, each iteration increases the likelihood of the data to some extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. To show that $\\left|\\hat{x_i}-\\hat{x_j}\\right|=\\left|z_i-z_j\\right|$, we can start by expanding $\\left|\\hat{x_i}-\\hat{x_j}\\right|$ as follows:\n",
    "\\begin{align*}\n",
    "\\left|\\hat{x_i}-\\hat{x_j}\\right| &= \\left|V_{1: k} z_i - V_{1: k} z_j\\right| \\\n",
    "&= \\left|V_{1: k} (z_i - z_j)\\right| \\\n",
    "&= \\sqrt{(V_{1: k} (z_i - z_j))^T (V_{1: k} (z_i - z_j))}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    " \\\n",
    "&= \\sqrt{(z_i - z_j)^T V_{1: k}^T V_{1: k} (z_i - z_j)} \\\n",
    "&= \\sqrt{(z_i - z_j)^T (V_{1: k}^T V_{1: k}) (z_i - z_j)} \\\n",
    "&= \\sqrt{(z_i - z_j)^T I_k (z_i - z_j)} \\\n",
    "&= \\sqrt{(z_i - z_j)^T (z_i - z_j)} \\\n",
    "&= \\left|z_i-z_j\\right|.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, $\\left|\\hat{x_i}-\\hat{x_j}\\right|=\\left|z_i-z_j\\right|$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing from the previous derivation, we have:\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n\\left|x_i-\\hat{x_i}\\right|2^2 &= \\sum{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\left(\\sum_{i=1}^nx_i^Tv_j\\right)^2/\\left|v_j\\right|2^2 \\\n",
    "&= \\sum{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{(\\sum_{i=1}^nx_i^Tv_j)^2}{\\lambda_j} \\\n",
    "&= \\sum_{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left(\\sum_{i=1}^n(x_i^Tv_j)^2\\right)}{\\lambda_j} \\\n",
    "&= \\sum_{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left|Xv_j\\right|_2^2}{\\lambda_j},\n",
    "\\end{align*}\n",
    "where $X$ is the $n \\times p$ matrix of observations and $\\lambda_j$ is the $j$-th eigenvalue of the covariance matrix.\n",
    "\n",
    "Now, we can use the property that the sum of eigenvalues equals the trace of the covariance matrix, i.e., $\\sum_{j=1}^p \\lambda_j = \\mathrm{tr}(S)$, where $S$ is the covariance matrix. Since $\\mathrm{tr}(S) = \\sum_{i=1}^n x_i^Tx_i$, we have:\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n\\left|x_i-\\hat{x_i}\\right|2^2 &= \\sum{i=1}^nx_i^Tx_i - \\sum_{j=1}^k\\frac{\\left|Xv_j\\right|2^2}{\\lambda_j} \\\n",
    "&= \\sum{j=1}^p\\frac{\\left|Xv_j\\right|2^2}{\\lambda_j} - \\sum{j=1}^k\\frac{\\left|Xv_j\\right|2^2}{\\lambda_j} \\\n",
    "&= \\sum{j=k+1}^p\\frac{\\left|Xv_j\\right|2^2}{\\lambda_j} \\\n",
    "&= \\sum{j=k+1}^p\\lambda_j\\frac{\\left|v_j\\right|2^2}{\\lambda_j} \\\n",
    "&= (n-1) \\sum{j=k+1}^p\\lambda_j.\n",
    "\\end{align*}\n",
    "This proves the desired result.\n",
    "\n",
    "Therefore, as we increase the number of principal components $k$, the reconstruction error decreases because we are including more information about the data. However, including too many principal components can lead to overfitting and a higher reconstruction error on new data. The optimal number of principal components to use depends on the specific problem and can be determined using techniques such as cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
